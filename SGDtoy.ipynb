{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent in Neural Network\n",
    "## Working with 2dimensions of data; (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dependecies, only numpy for now\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sigmoidfunct.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Activation function as sigmoid. Defining the sigmoid\n",
    "# when deriv is False, it is feedforward\n",
    "# when deriv is True, it is backpropagate, the derivation of sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv == True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input dataset\n",
    "X = np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])\n",
    "\n",
    "# Target or output dataset\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "# Seed random number to make it deterministic for future validation\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple neural network\n",
    "### 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly weights initialization\n",
    "weight_1 = 2*np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight_1 after 100000 times is \n",
      " [[  2.08166817e-16]\n",
      " [  2.22044605e-16]\n",
      " [ -3.05311332e-16]]\n",
      "Desired output is \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Calcualted outputs after 100000 times is \n",
      " [[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Iteration\n",
    "xrange = np.arange(100000)\n",
    "\n",
    "# Neural Network\n",
    "for iter in xrange:\n",
    "    \n",
    "    # forward propagation\n",
    "    layer_0 = X\n",
    "    layer_1 = nonlin(np.dot(layer_0, weight_1))\n",
    "    \n",
    "    # Error Calculation for layer_1\n",
    "    layer_1_err = layer_1 - y\n",
    "    \n",
    "    # Backpropagation when (deriv=True)\n",
    "    # multiply the error with the 'derivative of activation function'\n",
    "    layer_1_der = layer_1_err * nonlin(layer_1, True)\n",
    "    weight_slope = np.dot(layer_0.T, layer_1_der)\n",
    "    \n",
    "    # update the weights\n",
    "    weight_1 -= weight_slope\n",
    "    \n",
    "print (\"Weight_1 after {} times is \\n {}\".format(len(xrange), weight_1))\n",
    "print (\"Desired output is \\n {}\".format(y))\n",
    "print (\"Calcualted outputs after {} times is \\n {}\".format(len(xrange), layer_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neural network with SGD and alpha\n",
    "### with 2 hidden\n",
    "1. Forward propagation for all layers\n",
    "2. Backpropagation by\n",
    "    (1) Error calculation of the most right layer\n",
    "    (2) Derivation of that layer (error*derivation(weight))\n",
    "    (3) Step (1) but for second most right layer\n",
    "    (4) step (2) but for second most right layer\n",
    "    (5) Follow the sequence for all remaining layers\n",
    "    \n",
    "#### The forward propagation is easy because we don't need error calculation. While backpropagation has to be done step by step for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization of alpha\n",
    "alphas = [0.001, 0.01, 0.1, 0.5, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2layers.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training with alpha 0.001\n",
      "Error after 0 iterations 0.49641003190272537\n",
      "Error after 10000 iterations 0.49516402549338606\n",
      "Error after 20000 iterations 0.4935960431880486\n",
      "Error after 30000 iterations 0.4916063585594306\n",
      "Error after 40000 iterations 0.48910016654420474\n",
      "Error after 50000 iterations 0.48597785784615843\n",
      "\n",
      " Training with alpha 0.01\n",
      "Error after 0 iterations 0.49641003190272537\n",
      "Error after 10000 iterations 0.45743107444190134\n",
      "Error after 20000 iterations 0.359097202563399\n",
      "Error after 30000 iterations 0.23935813715897253\n",
      "Error after 40000 iterations 0.1430706590133703\n",
      "Error after 50000 iterations 0.09859642980892719\n",
      "\n",
      " Training with alpha 0.1\n",
      "Error after 0 iterations 0.49641003190272537\n",
      "Error after 10000 iterations 0.042888017000115755\n",
      "Error after 20000 iterations 0.02409899422852161\n",
      "Error after 30000 iterations 0.018110652146797843\n",
      "Error after 40000 iterations 0.014987616272210912\n",
      "Error after 50000 iterations 0.013014490538142586\n",
      "\n",
      " Training with alpha 0.5\n",
      "Error after 0 iterations 0.49641003190272537\n",
      "Error after 10000 iterations 0.013016237730678237\n",
      "Error after 20000 iterations 0.00858428907671557\n",
      "Error after 30000 iterations 0.0068039935348322995\n",
      "Error after 40000 iterations 0.005789845803016303\n",
      "Error after 50000 iterations 0.0051168472993909885\n",
      "\n",
      " Training with alpha 1\n",
      "Error after 0 iterations 0.49641003190272537\n",
      "Error after 10000 iterations 0.008584525653247159\n",
      "Error after 20000 iterations 0.0057894598625078085\n",
      "Error after 30000 iterations 0.004629176776769985\n",
      "Error after 40000 iterations 0.003958765280273649\n",
      "Error after 50000 iterations 0.0035101225678616766\n",
      "\n",
      " Training with alpha 10\n",
      "Error after 0 iterations 0.49641003190272537\n",
      "Error after 10000 iterations 0.003129388763011837\n",
      "Error after 20000 iterations 0.002144595579852179\n",
      "Error after 30000 iterations 0.0017239754995622308\n",
      "Error after 40000 iterations 0.0014782145122908034\n",
      "Error after 50000 iterations 0.0013127406283356764\n",
      "\n",
      " Training with alpha 100\n",
      "Error after 0 iterations 0.49641003190272537\n",
      "Error after 10000 iterations 0.12547698383358538\n",
      "Error after 20000 iterations 0.12533033354043677\n",
      "Error after 30000 iterations 0.12526772879373652\n",
      "Error after 40000 iterations 0.12523107370012884\n",
      "Error after 50000 iterations 0.12520635280373757\n"
     ]
    }
   ],
   "source": [
    "# Iteration with alpha\n",
    "for alpha in alphas:\n",
    "    print (\"\\n Training with alpha {}\".format(alpha))\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # Randomly weights initialization\n",
    "    weight_1 = 2*np.random.random((3,4)) - 1\n",
    "    weight_2 = 2*np.random.random((4,1)) - 1\n",
    "    \n",
    "    # iteration\n",
    "    krange = np.arange(60000)\n",
    "    \n",
    "    # Neural network with different alpha effect\n",
    "    for j in krange:\n",
    "        \n",
    "        ## Forward propagation step ##\n",
    "        layer_0 = X\n",
    "        layer_1 = nonlin(np.dot(layer_0, weight_1))\n",
    "        layer_2 = nonlin(np.dot(layer_1, weight_2))\n",
    "        \n",
    "        ## Backpropagation step ## \n",
    "        # Step (1)\n",
    "        layer_2_err = layer_2 - y\n",
    "        \n",
    "        # To output error calculation for every 10000 iterations \n",
    "        if (j% 10000) == 0:\n",
    "            print(\"Error after {} iterations {}\".format(j, np.mean(np.abs(layer_2_err))))\n",
    "        \n",
    "        # Step (2)\n",
    "        layer_2_der = layer_2_err * nonlin(layer_2, True)\n",
    "        \n",
    "        # Step (3)\n",
    "        layer_1_err = layer_2_der.dot(weight_2.T)\n",
    "        \n",
    "        # Step (4)\n",
    "        layer_1_der = layer_1_err * nonlin(layer_1, True) \n",
    "        \n",
    "        # update the weights\n",
    "        weight_2 -= alpha * (layer_1.T.dot(layer_2_der))\n",
    "        weight_1 -= alpha * (layer_0.T.dot(layer_1_der))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
